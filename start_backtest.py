"""
ç»Ÿä¸€å›æµ‹å¯åŠ¨å…¥å£
Run comprehensive backtest based on backtest_config.json configuration.

ä½¿ç”¨æ–¹æ³•:
    python start_backtest.py

é…ç½®æ–‡ä»¶: backtest_config.json
"""
import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

from src.backtest.engine import backtest_strategies
from src.backtest.report import print_summary_report, create_comparison_table
from src.analysis.scorers import SimpleScorer, EnhancedScorer
from src.analysis.exiters import ATRExiter, LayeredExiter

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


class OutputRedirector:
    """é‡å®šå‘printè¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°"""
    def __init__(self, filepath):
        self.terminal = sys.stdout
        self.log = open(filepath, 'w', encoding='utf-8')
    
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
    
    def flush(self):
        self.terminal.flush()
        self.log.flush()
    
    def close(self):
        self.log.close()


def load_config(config_path: str = "backtest_config.json") -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config


def parse_strategies(strategy_configs: list) -> list:
    """è§£æç­–ç•¥é…ç½®"""
    scorer_map = {
        'SimpleScorer': SimpleScorer,
        'EnhancedScorer': EnhancedScorer
    }
    
    exiter_map = {
        'ATRExiter': ATRExiter,
        'LayeredExiter': LayeredExiter
    }
    
    strategies = []
    for config in strategy_configs:
        scorer_class = scorer_map.get(config['scorer'])
        exiter_class = exiter_map.get(config['exiter'])
        
        if scorer_class and exiter_class:
            strategies.append((scorer_class(), exiter_class()))
        else:
            logger.warning(f"Unknown strategy: {config['scorer']} + {config['exiter']}")
    
    return strategies


def main():
    """Run backtest based on config file."""
    # Load environment
    load_dotenv()
    api_key = os.getenv('JQUANTS_API_KEY')
    
    if not api_key:
        logger.error("JQUANTS_API_KEY not found in environment")
        return
    
    # Load configuration
    try:
        config = load_config("backtest_config.json")
    except FileNotFoundError:
        logger.error("backtest_config.json not found! Creating default config...")
        # Create default config if missing
        default_config = {
            "backtest_config": {
                "tickers": ["7203", "6501", "8035"],
                "start_date": "2021-01-01",
                "end_date": "2026-01-08",
                "starting_capital_jpy": 5000000,
                "include_benchmark": True,
                "strategies": [
                    {"scorer": "SimpleScorer", "exiter": "ATRExiter"},
                    {"scorer": "SimpleScorer", "exiter": "LayeredExiter"},
                    {"scorer": "EnhancedScorer", "exiter": "ATRExiter"},
                    {"scorer": "EnhancedScorer", "exiter": "LayeredExiter"}
                ]
            },
            "output_config": {
                "save_to_file": True,
                "output_dir": "backtest_results",
                "include_timestamp": True
            }
        }
        with open("backtest_config.json", 'w', encoding='utf-8') as f:
            json.dump(default_config, f, indent=2, ensure_ascii=False)
        config = default_config
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON in backtest_config.json: {e}")
        return
    
    # Parse config
    backtest_cfg = config['backtest_config']
    output_cfg = config.get('output_config', {
        'save_to_file': True,
        'output_dir': 'backtest_results',
        'include_timestamp': True
    })
    
    tickers = backtest_cfg['tickers']
    strategies = parse_strategies(backtest_cfg['strategies'])
    start_date = backtest_cfg['start_date']
    end_date = backtest_cfg['end_date']
    starting_capital = backtest_cfg['starting_capital_jpy']
    include_benchmark = backtest_cfg['include_benchmark']
    
    # Setup output redirection
    output_file = None
    redirector = None
    
    if output_cfg.get('save_to_file', True):
        output_dir = Path(output_cfg.get('output_dir', 'backtest_results'))
        output_dir.mkdir(exist_ok=True)
        
        if output_cfg.get('include_timestamp', True):
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = output_dir / f"backtest_result_{timestamp}.txt"
        else:
            output_file = output_dir / "backtest_result.txt"
        
        redirector = OutputRedirector(output_file)
        sys.stdout = redirector
        logger.info(f"è¾“å‡ºå°†ä¿å­˜åˆ°: {output_file}")
    
    print("\n" + "="*80)
    print("å›æµ‹é…ç½®")
    print("="*80)
    print(f"é…ç½®æ–‡ä»¶: backtest_config.json")
    print(f"è‚¡ç¥¨ä»£ç : {tickers} ({len(tickers)} åª)")
    print(f"ç­–ç•¥ç»„åˆ: {len(strategies)} ä¸ª")
    print(f"æ€»å›æµ‹æ•°: {len(tickers) * len(strategies)}")
    print(f"å›æµ‹æœŸé—´: {start_date} è‡³ {end_date}")
    print(f"èµ·å§‹èµ„é‡‘: Â¥{starting_capital:,}")
    print(f"åŒ…å«TOPIXåŸºå‡†: {'æ˜¯' if include_benchmark else 'å¦'}")
    if output_file:
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print("="*80)
    
    # User confirmation if running many backtests
    total_runs = len(tickers) * len(strategies)
    if total_runs > 10:
        response = input(f"\nå°†è¿è¡Œ {total_runs} ä¸ªå›æµ‹ï¼Œé¢„è®¡è€—æ—¶ {total_runs // 4} åˆ†é’Ÿã€‚ç»§ç»­? (y/n): ")
        if response.lower() != 'y':
            print("å›æµ‹å·²å–æ¶ˆã€‚")
            if redirector:
                sys.stdout = redirector.terminal
                redirector.close()
            return
    
    print("\nå¼€å§‹å›æµ‹...\n")
    
    # Run backtest
    try:
        results_df = backtest_strategies(
            tickers=tickers,
            strategies=strategies,
            start_date=start_date,
            end_date=end_date,
            starting_capital_jpy=starting_capital,
            include_benchmark=include_benchmark
        )
        
        # Save results to CSV
        csv_file = f"backtest_results_{end_date.replace('-', '')}.csv"
        results_df.to_csv(csv_file, index=False)
        logger.info(f"Results saved to {csv_file}")
        
        # Print summary report
        # Convert DataFrame back to BacktestResult objects for reporting
        from src.backtest.models import BacktestResult
        results_list = []
        for _, row in results_df.iterrows():
            result = BacktestResult(
                ticker=row['ticker'],
                ticker_name=row['ticker_name'],
                scorer_name=row['scorer_name'],
                exiter_name=row['exiter_name'],
                start_date=row['start_date'],
                end_date=row['end_date'],
                starting_capital_jpy=row['starting_capital_jpy'],
                final_capital_jpy=row['final_capital_jpy'],
                total_return_pct=row['total_return_pct'],
                annualized_return_pct=row['annualized_return_pct'],
                sharpe_ratio=row['sharpe_ratio'],
                max_drawdown_pct=row['max_drawdown_pct'],
                num_trades=row['num_trades'],
                win_rate_pct=row['win_rate_pct'],
                avg_gain_pct=row['avg_gain_pct'],
                avg_loss_pct=row['avg_loss_pct'],
                avg_holding_days=row['avg_holding_days'],
                profit_factor=row['profit_factor'],
                benchmark_return_pct=row.get('benchmark_return_pct'),
                alpha=row.get('alpha'),
                beat_benchmark=row.get('beat_benchmark'),
                beta=row.get('beta'),
                tracking_error=row.get('tracking_error'),
                information_ratio=row.get('information_ratio')
            )
            results_list.append(result)
        
        # Print detailed results
        print("\n" + "="*80)
        print("è¯¦ç»†å›æµ‹ç»“æœ")
        print("="*80)
        
        for result in results_list:
            print(result.to_summary_string())
            print()
        
        # Print comparison table
        print("\nç­–ç•¥å¯¹æ¯”:")
        print("-"*80)
        comparison_cols = ['ticker', 'ticker_name', 'scorer_name', 'exiter_name', 
                          'total_return_pct', 'sharpe_ratio', 'max_drawdown_pct', 
                          'num_trades', 'win_rate_pct']
        if 'alpha' in results_df.columns:
            comparison_cols.extend(['alpha', 'beta', 'information_ratio', 'beat_benchmark'])
        
        print(results_df[comparison_cols].to_string(index=False))
        
        # Find winner
        best = max(results_list, key=lambda r: r.sharpe_ratio)
        print("\n" + "="*80)
        print(f"ğŸ† æœ€ä½³ç­–ç•¥: {best.ticker} Ã— {best.scorer_name} + {best.exiter_name}")
        print(f"   å¤æ™®æ¯”ç‡: {best.sharpe_ratio:.2f}")
        print(f"   æ€»å›æŠ¥: {best.total_return_pct:+.2f}%")
        if best.alpha is not None:
            print(f"   Alpha: {best.alpha:+.2f}%")
            if best.beta is not None:
                print(f"   Beta: {best.beta:.2f}")
            if best.information_ratio is not None:
                print(f"   ä¿¡æ¯æ¯”ç‡: {best.information_ratio:.2f}")
        print(f"   æœ€å¤§å›æ’¤: {best.max_drawdown_pct:.2f}%")
        print("="*80 + "\n")
        
        logger.info("å›æµ‹å®Œæˆ!")
        
        if output_file:
            print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            print(f"âœ… CSVå·²ä¿å­˜åˆ°: {csv_file}")
        
    except Exception as e:
        logger.error(f"å›æµ‹å¤±è´¥: {e}", exc_info=True)
    
    finally:
        # Restore stdout and close file
        if redirector:
            sys.stdout = redirector.terminal
            redirector.close()
            print(f"\nâœ… è¾“å‡ºå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == '__main__':
    main()
